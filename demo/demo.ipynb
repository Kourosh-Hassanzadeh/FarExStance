{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1522113",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d6aa44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at HooshvareLab/bert-base-parsbert-ner-uncased were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "IOStream.flush timed out\n"
     ]
    }
   ],
   "source": [
    "# inference_demo.ipynb\n",
    "\n",
    "# ## FarExStance Demo Inference Notebook\n",
    "# \n",
    "# This notebook demonstrates running the XLM-R model for predicting stance and generating explanations\n",
    "# on a small 5-sample dataset. All analyses and plots are included in the notebook.\n",
    "\n",
    "# ### 1. Import Libraries\n",
    "import torch\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sys\n",
    "\n",
    "root_path = Path(\"..\").resolve()\n",
    "sys.path.append(str(root_path))\n",
    "\n",
    "# Import repo modules\n",
    "from common.dataset import DatasetInitModel, StanceDataset\n",
    "from common.preprocessor import Preprocessor\n",
    "from common.torch_trainer import TorchTrainer\n",
    "from common.load_transformers import LoadTransformers\n",
    "from models_architecture.TransformerWithExtraFeatures import TransformerWithExtraFeatures\n",
    "\n",
    "# Logging function\n",
    "import logging\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.INFO)\n",
    "def print_log(*args):\n",
    "    for arg in args:\n",
    "        print(arg)\n",
    "        logger.info(arg)\n",
    "\n",
    "# ### 2. Set Device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print_log(f\"Device: {device}\")\n",
    "\n",
    "# ### 3. Model and Dataset Parameters\n",
    "ner_model_id = 'HooshvareLab/bert-base-parsbert-ner-uncased'\n",
    "dataset_labels = {'claim_name': \"claim\", 'text_name': \"content\",'label_name': \"stance\"}\n",
    "\n",
    "dataset_params = {\n",
    "    'padding' : True, 'truncation': True, 'max_length': 512,\n",
    "    'claim_max_length': 38,\n",
    "    'device': device,\n",
    "    'similarity_features_no_for_start': 5,\n",
    "    'similarity_features_no_for_end': 2,\n",
    "    'ner_features_no': 8,\n",
    "    'remove_dissimilar_sentences': True,\n",
    "    'similar_sentence_no': 8,\n",
    "    'ner_model_path': ner_model_id,\n",
    "    'ner_tokenizer_path': ner_model_id,\n",
    "    'ner_none_token_id': 0,\n",
    "    'sentence_similarity_model_path': \"sentence-transformers/all-MiniLM-L12-v2\",\n",
    "    'log_function': print_log,\n",
    "    'save_load_features_path': 'demo/data/features',\n",
    "    'remove_news_agency_name': False\n",
    "}\n",
    "\n",
    "transformer_params = {\n",
    "    'model_loader': LoadTransformers.xlm_roberta,\n",
    "    'transformer_model_path': \"FacebookAI/xlm-roberta-base\"\n",
    "}\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "vectorizer = AutoTokenizer.from_pretrained(transformer_params[\"transformer_model_path\"])\n",
    "transformer_params[\"vectorizer\"] = vectorizer\n",
    "\n",
    "batch_size = 2\n",
    "accumulation_steps = 1\n",
    "\n",
    "# ### 4. Load Small 5-Sample Dataset\n",
    "demo_dataset_path = \"/home/kourosh/FarExStance/demo/data/new_test.xlsx\"\n",
    "df_demo = pd.read_excel(demo_dataset_path)\n",
    "df_demo.head()\n",
    "\n",
    "# ### 5. Create Dataset and DataLoader\n",
    "test_dataset_input = {'data_set_path': demo_dataset_path, 'preprocessor': Preprocessor()}\n",
    "\n",
    "init_params = DatasetInitModel(\n",
    "    **test_dataset_input,\n",
    "    **dataset_labels,\n",
    "    **dataset_params,\n",
    "    **transformer_params\n",
    ")\n",
    "\n",
    "test_dataset = StanceDataset(init_params)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "print_log(f\"Test dataset length: {len(test_dataloader.dataset)}\")\n",
    "\n",
    "# ### 6. Load Model\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "trainer = TorchTrainer(None, None, None, None, loss_fn, device, accumulation_steps, print_log)\n",
    "\n",
    "model_save_directory = \"models/\"\n",
    "epoch_model_name = \"/epoch5_loss1.3054144382476807.pt\"\n",
    "trainer.test_dataloader = test_dataloader\n",
    "\n",
    "loss, result_metrics, pred_labels = trainer.test_model(model_save_directory + epoch_model_name)\n",
    "\n",
    "print_log(\"Loss:\", loss)\n",
    "print_log(\"Result metrics:\", result_metrics)\n",
    "\n",
    "# ### 7. Save Outputs\n",
    "df_demo[\"pred_stance\"] = pred_labels\n",
    "df_demo[\"gen_explanation\"] = test_dataset.explanation\n",
    "\n",
    "result_save_path = \"demo/results/\"\n",
    "Path(result_save_path).mkdir(parents=True, exist_ok=True)\n",
    "df_demo.to_excel(f\"{result_save_path}demo_results.xlsx\", index=False)\n",
    "df_demo.head()\n",
    "\n",
    "# ### 8. Analysis and Visualization\n",
    "# Plot the distribution of original stance labels\n",
    "plt.figure(figsize=(6,4))\n",
    "sns.countplot(data=df_demo, x=\"stance\")\n",
    "plt.title(\"Distribution of Original Labels (stance)\")\n",
    "plt.show()\n",
    "\n",
    "# Plot the distribution of predicted stance labels\n",
    "plt.figure(figsize=(6,4))\n",
    "sns.countplot(data=df_demo, x=\"pred_stance\")\n",
    "plt.title(\"Distribution of Model Predictions\")\n",
    "plt.show()\n",
    "\n",
    "# Confusion matrix comparing true vs predicted labels\n",
    "confusion_matrix = pd.crosstab(df_demo['stance'], df_demo['pred_stance'], rownames=['Actual'], colnames=['Predicted'])\n",
    "plt.figure(figsize=(6,5))\n",
    "sns.heatmap(confusion_matrix, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()\n",
    "\n",
    "# Plot histogram of explanation lengths\n",
    "df_demo['explanation_len'] = df_demo['gen_explanation'].apply(lambda x: len(str(x).split()))\n",
    "plt.figure(figsize=(6,4))\n",
    "sns.histplot(df_demo['explanation_len'], bins=5)\n",
    "plt.title(\"Distribution of Generated Explanation Length (words)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc9da57c",
   "metadata": {},
   "source": [
    "* if you have a good connection it will run"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env2 (3.10.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
